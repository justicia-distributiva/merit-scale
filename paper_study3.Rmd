---
title: "Measuring Perceptions and Preferences for Meritocracy"
css: "custom.css"
linestretch: '1.5'
link-citations: yes
output:
  bookdown::html_document2:
    number_sections: false
  bookdown::pdf_document2:
    template: null
    toc: false
linkcolor: blue
#bibliography:
editor_options:
  chunk_output_type: console
geometry: margin=0.78in
header-includes:
  # - \usepackage[spanish,es-tabla,es-nodecimaldot]{babel}
  - \usepackage{times}           # Times New Roman
  - \usepackage{caption}
  - \captionsetup[figure, table]{labelfont={bf},labelformat={default},labelsep=period}
  - \usepackage{graphicx}
  - \usepackage{float}
  - \usepackage{booktabs}
  - \usepackage{longtable}
  - \usepackage{array}
  - \usepackage{multirow}
  - \usepackage{wrapfig}
  - \usepackage{float}
  - \usepackage{colortbl}
  - \usepackage{pdflscape}
  - \usepackage{tabu}
  - \usepackage{threeparttable}
---

```{r eval=FALSE, include=FALSE}
# for render in pdf run rmarkdown::render_site("docs/paper.Rmd", output_format = "all")
# clean #in the yml
rmarkdown::render("docs/paper.Rmd", output_format = "bookdown::pdf_document2")
rmarkdown::render("docs/paper.Rmd", output_format = "bookdown::html_document2")
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE,message = FALSE, cache = FALSE,out.width = '85%',fig.pos= "H")
# knitr::opts_knit$set(base.url = "../") #relative path for .html output file
# knitr::opts_knit$set(root.dir = "../") #relative path for chunks within .rmd files
options(scipen=999)
rm(list=ls())
options(knitr.kable.NA = '')
options(knitr.graphics.error = FALSE)
Sys.setlocale("LC_ALL", "ES_ES.UTF-8")
```

```{r message=FALSE, warning=FALSE, include=FALSE, echo=FALSE}
#RUN THIS BEFORE THE ANALYSIS
pacman::p_load(knitr)
table_format = if(is_html_output()) {
  "html"
} else if(is_latex_output()) {
  "latex"
}

table_format2 = if(is_html_output()) {
  T
} else if(is_latex_output()) {
  F
}
```


## Study 3: Additional validity analyses

We performed two further analyses in order to add evidence regarding the quality of the scale. Firstly we analyzed the convergent validity by exploring the association of the merit scale with related concepts and its correspondent measurements. Secondly, we test the internal consistency of the measurement model behind the scale comparing with a different sample through measurement invariance procedures .

### Convergent validity

_Data_

For this analysis we examined data from one wave of the same study described above, which was a three-wave panel survey. The last wave of the study included additional measures that allow testing the correlations of the merit scale with some related constructs as opportunity beliefs and personal wherewithal. After listwise deletion of missing cases in socio-demographics information, a total of 1422 individuals took part in the third wave: 668 (46.97%) women and 754 (53.03%) men. The ages of 50.57% of them are 45 years or older, and 36.28% held a tertiary degree.

_Intruments_

The following scales were included for testing their correlations with the the 8-item meritocracy scale:

- Opportunity beliefs: We used two items of the social inequality module of the International Social Survey Programme. Studies use generally both indicators to measure meritocratic and non-meritocratic beliefs [@McCall2017Exposurerisinginequality; @mijs_paradox_2019].  One item measures the importance of individual factors in determining life outcomes by asking for the importance of hard work for getting ahead in life (M=3.76, SD=0.93). The second item asks for the importance of coming from a wealthy family and captures the importance of structural factors in getting ahead (M=2.75, SD=1.27). Participants responded to each item using a 1 (not important at all) to 5 (essential) response scale. We call the first item \textit{hard work} and the second one \textit{social origin}.

- Personal wherewithal: the questionnaire included The Neoliberal Beliefs Inventory (NBI), which considers four factors: Government Interference preferences, Competition preferences, System Inequality perception and Personal Wherewithal [@bay-chengTrackingHomoOeconomicus2015]. We used the factor _personal wherewithal_ that reflects meritocracy beliefs in terms of the importance of personal attributes as strength and skills to yield success. The construct consists of 8 items (e.g., “Any goal can be achieved with enough hard work and talent”, “I’ve benefited from working hard, so there’s no reason others can’t”, “Anyone who is willing to work hard can be succesful in Chile”). Participants answered the items using a 1 (totally disagree) to 6 (totally agree) scale. We simple computed mean scores, with higher scores indicating stronger support for meritocracy (M=3.32, SD=0.88, $\alpha$ = 0.89).

Both instruments cover mainly the perceptual side of the concept. Therefore, in general we expect larger correlation with meritocratic perceptions than with preferences. In the case of opportunity beliefs, the hard work item should correlate positively with meritocratic perception whereas the importance for coming from a wealthy family is expected to correlate positively with non-meritocratic perceptions. Regarding the second instrument of personal wherewithal, we also anticipate a positive correlation with meritocratic perception and low to null correlations with the other dimensions of the merit scale.

_Results_

We used polyserial correlations to evaluate relationships of meritocractic and non-meritocratic preferences and perceptions with the items of the opportunity beliefs battery. For personal wherewithal, we examined Pearson correlations.

Table \@ref(tab:val-conv) shows the polyserial and pearson correlations between our meritocracy scale and common measures of meritocracy. The findings provide evidence for convergent validity for the scale. As we expected, preferences show very weak correlations. Findings also indicate that correlations of perceptions with opportunity beliefs are consistent with our expectations. The item for social origin shows a positive and moderate association with unmeritocratic perception (r=.354), and a negative and low correlation with meritocratic perception (-0.197). Regarding the item of hard work, the correlation with meritocratic perception is positive but weak (r=.187). One explanation for this result is that the support for hard work is very high among respondents---only 8.6 percent indicates _not very important_ or _not important at all_.


```{r val-conv}
pacman::p_load(kableExtra, lavaan, dplyr,tidyr)

load(file="input/data/proc/data_s3_conv.RData")

#
model <- '
perc_merit =~  meritv03_perc_effort + meritv03_perc_talent
perc_nmerit=~ meritv03_perc_wpart + meritv03_perc_netw
pref_merit =~ meritv03_pref_effort + meritv03_pref_talent
pref_nmerit=~ meritv03_pref_wpart + meritv03_pref_netw

nbmerit =~  nb_merit_1 + nb_merit_2 + nb_merit_3 + nb_merit_4 + nb_merit_5 + nb_merit_6 + nb_merit_7 

perc_merit ~~  get_ah_1
perc_nmerit ~~ get_ah_1
pref_merit ~~ get_ah_1
pref_nmerit ~~ get_ah_1
nbmerit ~~ get_ah_1

perc_merit ~~  get_ah_5
perc_nmerit ~~ get_ah_5
pref_merit ~~ get_ah_5
pref_nmerit ~~ get_ah_5
nbmerit ~~ get_ah_5

perc_merit ~~  nbmerit
perc_nmerit ~~ nbmerit
pref_merit ~~ nbmerit
pref_nmerit ~~ nbmerit

'
fit3_o1=lavaan::cfa(model,data = dat04w3, estimator = "DWLS", ordered = c("meritv03_perc_effort","meritv03_perc_talent",
"meritv03_perc_wpart","meritv03_perc_netw","meritv03_pref_effort","meritv03_pref_talent","meritv03_pref_wpart","meritv03_pref_netw" ,"nb_merit_1" , "nb_merit_2" , "nb_merit_3" , "nb_merit_4" , "nb_merit_5" , "nb_merit_6" , "nb_merit_7","get_ah_1","get_ah_5"))


# extract correlation matrix from lavaan model
correlations = standardizedsolution(fit3_o1) %>% filter(op=="~~") %>% 
                dplyr::select(lhs,rhs,est.std) %>% 
                dplyr::filter(lhs== "perc_merit" | 
                             lhs== "pref_merit" | 
                             lhs== "perc_nmerit"| 
                             lhs== "pref_nmerit") %>%
                pivot_wider(names_from = "rhs", values_from = "est.std") %>% 
                dplyr::select(lhs,get_ah_1, get_ah_5, nbmerit) %>% 
                mutate(merit_scale=
                      case_when(lhs == "perc_merit" ~  "Meritocratic perception",
                                lhs=="perc_nmerit" ~ "Unmeritocratic perception", 
                                lhs =="pref_merit" ~ "Meritocratic preferences" , 
                                lhs =="pref_nmerit" ~ "Unmeritocratic preferences"))%>% 
                select(merit_scale, get_ah_1, get_ah_5,nbmerit)

# Make table 
kable(correlations,col.names = c("Merit-Scale","Social Origin ", "Hard Work " , "Wherewithal"), full_width = T, linesep = "", caption = c("Correlation whit other merit scales")) %>% kable_styling(
    full_width = T,
    position = "center",
    font_size = 14,
    bootstrap_options=c("striped", "bordered")) %>%
  column_spec(column = 1, width = "1.5cm", ) 


```


The measure of NBI for _personal wherewithal_ also provides evidence for the convergent validity of the merit scale. Results indicate a positive correlation of this measure with meritocratic preference (r=.392), suggesting that both variables capture similar concepts. The NBI's factor depicts a positive association with unmeritocratic preference (r=.208). This finding is consistent with literature, inasmuch as the belief for meritocray is associated with the justification of inequality [@McCall2017Exposurerisinginequality; @MadeiraPrimesConsequencesSystematic2019]. Furthermore, results of our confirmatory factor analysis indicate that meritocratic perceptions and unmeritocratic preferences show a positive and weak correlation.

### Measurement invariance

The modeling for invariance testing has been gaining more relevance in sociological survey studies, mainly due to the interest in the validity of measurement scales in comparative studies in various social and cultural contexts [@Davidovetal2014]. In this section we will use an analysis of invariance between groups, where the proposed factorial structure is expected to be independent of belonging to the sample corresponding to the original study, comparing it with another more recent study available and where the meritocracy scale was also incorporated.

The invariance measurement procedure consists of a series of nested models to which restrictions are progressively incorporated on the parameters of the measurement model. The literature generally suggests that this modeling should be done through four levels or types of progressive restriction [@Milfont2010; @millsap_Statistical_2011; @vandeschoot_checklist_2012]:

1. **Configural:** the model is estimated only indicating the factorial structure used in the CFA.

2. **Weak:** an equality restriction is applied to the factor loadings in the different groups, that is, the loadings are forced to be identical in both measurements.

3. **Strong:** equality restrictions are added to the intercepts of each indicator.

4. **Strict:** equality restrictions are added to the error variances of each indicator.

*Data*

For testing the invariance we compared the data described previously with a new data source coming from an online survey carried out during the first half of 2020. The characteristics of this sample in terms of application and coverage are equivalent to those of the previous study. The final sample obtained contains 1,242 cases, where 605 (48.71%) are women and 637 (51.28%) are men. 48.84% of them are 45 years of age or older, and 33.97% have tertiary education or higher.

_Instruments_

The items of the meritocracy scale were identical to the original. The items were administrated in a randomized order (which corresponds to the application modality of Group 3 present in Figure \@ref(fig: appmod)).

_Results_

Tabla \@ref(tab:fcloads-inv) shows de results of the CFA estimation of the meritocracy scale in this new sample. As observed, the loadings and the fit indices show similar results as the ones presented for the original study above, which gives a first base with wich to start the invariance analysis.

The first step for invariance testing is the estimation of the configural model, which serves as the baseline for further comparison and it is expected to adequately meet the global fit criteria of a measurement model. Although the chi-square statistic is used as a global measure of fit,since its sensitivity to sample size it is recommended to use three additional fit indices: Comparative Fit Index (CFI), which should have a value greater than 0.95; Root Mean Square Error Approximation (RMSEA), which must be in the range of values ​​of 0.05 and 0.08; and the Standardized Root Mean Square Residuals (SRMR) which must be less than 0.08 [@vandeschoot_checklist_2012]. Besides, the literature on measurement invariance suggests some complementary approaches for the evaluation of the fit, from which we will consider the incremental adjustment of the fit indexes [@cheung_Evaluating_2002; @Milfont2010; @dimitrov_Testing_2010] and the ANOVA test for means comparison in nested models [@newsom_Longitudinal_2015].



# Analysis

```{r invargroup}

#---Load packages and data -------------------------------------#

pacman::p_load(sjPlot,dplyr,lavaan,semPlot,semTools,stargazer,corrplot,psych,knitr,kableExtra,stats)


load(file = "input/data/proc/data_s3_inv.RData")

pov01 <- data_s3_inv %>% filter(dataset=="pvw01")
fs01  <- data_s3_inv %>% filter(dataset=="fsw02")

#-----Meassurement model--------------------------------------#



model01 <- '
perc_merit  =~ perc_effort + perc_talent
perc_nmerit =~ perc_wpart  + perc_netw
pref_merit  =~ pref_effort + pref_talent
pref_nmerit =~ pref_wpart  + pref_netw'


#-----Invariance---------------------------------------------#



inv01<- measurementInvariance(model=model01,data=data_s3_inv,group="dataset",estimator = "ML",strict=TRUE)

conf  <- inv01$fit.configural
weak  <- inv01$fit.loadings
strong<- inv01$fit.intercepts
strict<- inv01$fit.residuals

tab01<- anova(conf,weak,strong,strict,SB.classic=TRUE) %>% 
  as_tibble() %>% 
  select("Chisq","Df","chisq_diff"=`Chisq diff`,"df_diff"=`Df diff`,"pvalue"=`Pr(>Chisq)`) %>% 
  mutate(stars=gtools::stars.pval(pvalue),
         chisqt=paste0(round(Chisq,2)," (",Df,") "),
         decision=ifelse(pvalue>0.05,yes = "Accept",no = "Reject"),
         model=c("Configural","Weak","Strong","Strict")) 
 
fit.meas<- bind_rows(fitmeasures(inv01$fit.configural,output ="matrix")[c("chisq","df","cfi","rmsea","rmsea.ci.lower","rmsea.ci.upper"),],
                     fitmeasures(inv01$fit.loadings,  output ="matrix")[c("chisq","df","cfi","rmsea","rmsea.ci.lower","rmsea.ci.upper"),],
                     fitmeasures(inv01$fit.intercepts,output ="matrix")[c("chisq","df","cfi","rmsea","rmsea.ci.lower","rmsea.ci.upper"),],
                     fitmeasures(inv01$fit.residuals, output ="matrix")[c("chisq","df","cfi","rmsea","rmsea.ci.lower","rmsea.ci.upper"),])

# compute differences in chisq, df, cfi and rmsea (90%, lower.ci - upper.ci )
fit.meas<- fit.meas %>% 
  mutate(diff.chi2 = chisq    - lag(chisq,default = first(chisq)),
         diff.df   = df       - lag(df,   default = first(df)),
         diff.cfi  = cfi      - lag(cfi,  default = first(cfi)),
         diff.rmsea   = rmsea - lag(rmsea,default = first(rmsea))) %>%
  round(3) %>% 
  mutate(rmsea.ci=paste0(rmsea," \n ", "(",rmsea.ci.lower,"-",rmsea.ci.upper,")"))


tab.inv<- bind_cols(tab01,fit.meas) %>% 
  select(model,chisqt,cfi,rmsea.ci,diff.chi2,diff.df,diff.cfi,diff.rmsea,stars,decision) %>% 
  mutate(diff.chi2=paste0(diff.chi2," (",diff.df,") ",stars)) %>% 
  select(model,chisqt,cfi,rmsea.ci,diff.chi2,diff.cfi,diff.rmsea,decision)

#clean values
tab.inv[tab.inv == c("0 (0) ")] <- NA
tab.inv[tab.inv == c(0)] <- NA

### Check an alternative method
configural        <- cfa(model01, data=data_s3_inv, group = "dataset", estimator="ML")
weak.invariance   <- cfa(model01, data=data_s3_inv, group = "dataset",group.equal = "loadings", estimator="ML")
strong.invariance <- cfa(model01, data=data_s3_inv, group = "dataset",group.equal = c( "loadings", "intercepts"), estimator="ML")
strict.invariance <- cfa(model01, data=data_s3_inv, group = "dataset",group.equal = c( "loadings", "intercepts", "residuals"), estimator="ML")



table_format = if(is_html_output()) {
  "html"
} else if(is_latex_output()) {
  "latex"
}

col.nam <- c("Model","$\\chi^2 (\\text{df})$","CFI","RMSEA (90 CI)",
             "$\\Delta \\chi^2 (\\Delta \\text{df}$)","$\\Delta \\text{CFI}$","$\\Delta \\text{RMSEA}$","Decision")
footnote <- paste0("N = ","; Group 1, n = ",conf@Data@nobs[[1]],"; Group 2, n = ",conf@Data@nobs[[2]], ", **p < 0.001")

kable(tab.inv, col.names = col.nam, align = "l",
      booktabs=TRUE,format = table_format,escape = FALSE,
      caption = "Multiple Group meassurement invariance for Perceptions and Preferences for Meritocracy") %>%
  kable_styling(full_width = FALSE,latex_options = "HOLD_position") %>%
  footnote(general = footnote, footnote_as_chunk = T)

```


Table \@ref(tab:invargroup) shows the results of the measurement invariance estimation. When attending to the traditional invariance test of $\Delta \chi^2 (\Delta \text{df})$, the results  support the invariance at the strong level meaning that the fit of the factor model of the merit scale is equivalent across samples when constraining factor loadings and intercepts to being equal. Such result is considered in general as evidence of invariance [citar paper Fischer en SJR], as strict forms of measurement invariance rarely hold [citar van der Schoot et al]. Still, the comparability of latents means requires strict invariance which in this case does not hold when considering $\Delta \chi^2 (\Delta \text{df})$. Nevertheless, the criteria of $\Delta \text{CFI}$ used for comparing models is close to the rejection criteria of >.01, whereas the $\Delta \text{RMSEA}$ fulfills the requirements of being below of the cut-off criteria as suggested by [ @chen 2007 ]. Therefore, using this last standard the level for strict invariance holds for the meritocracy scale.
